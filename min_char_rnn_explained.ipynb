{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyashkar-ml/Pytorch-learning/blob/main/min_char_rnn_explained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlXM8ekWbvFx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/input.txt', 'r') as f:\n",
        "  data = f.read()"
      ],
      "metadata": {
        "id": "wrfAWhXdccYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print(f\"data has {data_size} characters, {vocab_size} unique.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnWrBFE8cmlX",
        "outputId": "025c4d23-7778-4f08-83e3-80122189f051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 1115394 characters, 65 unique.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Each character in the vocabulary gets a unique integer index assigned, in the\n",
        "# half-open interval [0:N). These indices are useful to create one-hot encoded\n",
        "# vectors to represent characters in numerical computations.\n",
        "char_to_ix = {ch:i for i, ch in enumerate(chars)}\n",
        "ix_to_char = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "print('char_to_ix', char_to_ix)\n",
        "print('ix_to_char', ix_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0gZlVSSczNA",
        "outputId": "f8d523d8-9212-45f2-a97b-69af8614e9b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_to_ix {'w': 0, 'O': 1, 'i': 2, 'M': 3, 'n': 4, 'd': 5, 'T': 6, 'q': 7, 'Q': 8, '3': 9, 'o': 10, 'l': 11, 'J': 12, 'L': 13, '.': 14, 'U': 15, ';': 16, 'G': 17, 'p': 18, \"'\": 19, 'f': 20, '!': 21, 'z': 22, 'b': 23, '$': 24, 'x': 25, 'A': 26, 'H': 27, 'W': 28, 'm': 29, 'r': 30, 'a': 31, ':': 32, 'V': 33, ',': 34, 'g': 35, 'D': 36, 'P': 37, 'X': 38, 'F': 39, 'e': 40, 'j': 41, 'h': 42, 'I': 43, '\\n': 44, 'C': 45, 'Y': 46, 't': 47, '&': 48, 'R': 49, 'B': 50, '?': 51, ' ': 52, 's': 53, 'E': 54, 'k': 55, 'v': 56, '-': 57, 'y': 58, 'S': 59, 'K': 60, 'c': 61, 'u': 62, 'N': 63, 'Z': 64}\n",
            "ix_to_char {0: 'w', 1: 'O', 2: 'i', 3: 'M', 4: 'n', 5: 'd', 6: 'T', 7: 'q', 8: 'Q', 9: '3', 10: 'o', 11: 'l', 12: 'J', 13: 'L', 14: '.', 15: 'U', 16: ';', 17: 'G', 18: 'p', 19: \"'\", 20: 'f', 21: '!', 22: 'z', 23: 'b', 24: '$', 25: 'x', 26: 'A', 27: 'H', 28: 'W', 29: 'm', 30: 'r', 31: 'a', 32: ':', 33: 'V', 34: ',', 35: 'g', 36: 'D', 37: 'P', 38: 'X', 39: 'F', 40: 'e', 41: 'j', 42: 'h', 43: 'I', 44: '\\n', 45: 'C', 46: 'Y', 47: 't', 48: '&', 49: 'R', 50: 'B', 51: '?', 52: ' ', 53: 's', 54: 'E', 55: 'k', 56: 'v', 57: '-', 58: 'y', 59: 'S', 60: 'K', 61: 'c', 62: 'u', 63: 'N', 64: 'Z'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters for RNN\n",
        "hidden_size = 100       # size of hidden layer of neurons\n",
        "seq_length = 16         # number of steps to unroll the RNN for\n",
        "learning_rate = 1e-1"
      ],
      "metadata": {
        "id": "crlbVc8FdpVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop when processed this much data\n",
        "MAX_DATA = 1000000"
      ],
      "metadata": {
        "id": "o_dVFozfd6PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters/weights -- these are shared among all steps.\n",
        "# Weights initialized randomly; biases initialized to 0.\n",
        "# Inputs are characters one-hot encoded in a vocab-sized vector.\n",
        "# Dimensions: H = hidden_size, V = vocab_size\n",
        "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01       # input to hidden\n",
        "Whh = np.random.randn(hidden_size, hidden_size) * 0.01      # hidden to hidden\n",
        "Why = np.random.randn(vocab_size, hidden_size) * 0.01       # hidden to output\n",
        "bh = np.zeros((hidden_size, 1))                 # hidden bias\n",
        "by = np.zeros((vocab_size, 1))                  # output bias"
      ],
      "metadata": {
        "id": "FtGhZ6NdeFfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Overview of RNN**\n",
        "Recurrent Neural Networks are at the core an attempt to develop an internal structure that is appropriate for a particular task domain using internal 'hidden' units which are not part of the input or output vectors.\n",
        "\n",
        "Learning becomes more interesting but more difficult when we introduce hidden units whose actual desired states are not specified by the task. The simplest for of the learning procedure is for layered networks which have a layer of inputs at the bottom; any number of intermediate layers; and a layer of output units at the top.\n",
        "\n",
        "An input vector is presented to the network by setting the states of the input units.\n",
        "\n",
        "Then the stats of the units in each layer are determined by applying steps as followed for input vector $ x_t $ :\n",
        "- **Hidden State Calculation:**\n",
        "</br> $ h_t = \\tanh(W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h) $\n",
        "\n",
        "- **Output and Softmax:**\n",
        "<br> $ y_t = W_{hy} \\cdot h_t + b_y $\n",
        "<br> $ p_t = \\frac{ \\exp(y_t) }{ \\sum exp(y_t) } $ </br>\n",
        "\n",
        "where, $ h_{t-1} $ represents the hidden state input from previous states, $ b_h $ represents the biased term in hidden state calculation, and $ p_t $ represents the softmax output from the output vector $ y_t $.\n",
        "\n",
        "RNNs are particularly effective in tasks like language modeling, machine translation, etc. where the context of previous characters are crucial for predicting the next one.\n"
      ],
      "metadata": {
        "id": "S-SpN9hZ7BzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's work through min-char-rnn code one-step at a time\n",
        "\n",
        "### `lossFun` Function:\n",
        "This function runs both forward and backward passes through the RNN and computes the loss and gradients.\n",
        "\n",
        "```python\n",
        "def lossFun(inputs, targets, hprev):\n",
        "  \"\"\"\n",
        "  Runs forward and backward passes through the RNN.\n",
        "\n",
        "  inputs, targets: Lists of integers. For some i, inputs[i] is the input\n",
        "                   character (encoded as an index to the ix_to_char map) and\n",
        "                   targets[i] is the corresponding next character in the\n",
        "                   training data (similarly encoded).\n",
        "  hprev: Hx1 array of initial hidden state.\n",
        "  returns: loss, gradients on model parameters, and last hidden state.\n",
        "  \"\"\"\n",
        "```\n",
        "**Inputs**:\n",
        "- `inputs`: Indices representing the input characters.\n",
        "- `targets`: Indices representing the next characters in the sequence\n",
        "- `hprev`: Initial hidden state from the previous sequence\n",
        "\n",
        "**Outputs**:\n",
        "- `loss`: Cross-entropy loss\n",
        "- Gradients for the weights and biases (`dWxh, dWhh, dWhy, dbh, dby`)\n",
        "- The last hidden state (`hs[len(inputs)-1]`)\n",
        "\n"
      ],
      "metadata": {
        "id": "IU0CLL2gF8JO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Forward Pass\n",
        "\n",
        "The forward pass computes the hidden states and the outputs at each time step.\n",
        "\n",
        "```python\n",
        "# Initialize storage for variables needed for forward and backward passes\n",
        "xs, hs, ys, ps = {}, {}, {}, {}\n",
        "hs[-1] = np.copy(hprev) # Initialize with the given hidden state\n",
        "loss = 0\n",
        "```\n",
        "For each time step $ t $:\n",
        "1. **Input Encoding**: The input characters are converted into one-hot encoding vectors for input into the model.\n",
        "```python\n",
        "xs[t] = np.zeros((vocab_size, 1)) # one-hot encoding\n",
        "xs[t][inputs[t]] = 1\n",
        "```\n",
        "2. **Hidden State Calculation**: The hidden states to capture the task information and develop/emulate an internal structure is computed using:\n",
        "$$ h_t = \\tanh(W_{xh}\\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h) $$\n",
        "```python\n",
        "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)\n",
        "```\n",
        "3. **Output and Softmax**: We first calculate the unnormalized scores (`ys[t]`) and then converet those into softmax probabilities (`ps[t]`) for output:\n",
        "$$ y_t = W_{hy} \\cdot h_t + b_y $$\n",
        "$$ p_t = \\frac{\\exp(y_t)}{\\sum \\exp(y_t)} $$\n",
        "```python\n",
        "ys[t] = np.dot(Why, hs[t]) + by\n",
        "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
        "```\n",
        "4. **Loss Calculation**: The cross-entropy loss at each time step is then added up using:\n",
        "```python\n",
        "loss += -np.log(ps[t][targets[t],0])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "REdCbAsFF8sx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Backpropagation through time (BPTT)\n",
        "\n",
        "1. **Gradient of Loss w.r.t Softmax Output (`ps[t]`):**\n",
        "For each time step t:\n",
        "The loss at time step $ t $ is given by:\n",
        "$$ Loss_t = - \\log(p_{t,target}) $$\n",
        "The derivative of the loss w.r.t softmax probabilities $p_t$ is:\n",
        "$$ \\frac{\\partial Loss_t}{\\partial p_t} = p_t - 1_{target} $$\n",
        "Where:\n",
        "- $ p_t $ is the softmax probability vector at time step t.\n",
        "- $ 1_{target} $ is a one-hot vector with a 1 at the index of the target character.\n",
        "```python\n",
        "dy = np.copy(ps[t])\n",
        "dy[targets[t]] -= 1\n",
        "```\n",
        "\n",
        "2. **Gradient w.r.t Output Weights (`Why`) and Bias (`by`):**\n",
        "The output $ y_t $ at each time step is computed as:\n",
        "$$ y_t = W_{hy} \\cdot h_t + b_y $$\n",
        "The gradients of the loss w.r.t the weights and biases are given by:\n",
        "$$ \\frac{ \\partial Loss_t }{\\partial W_{hy}} =\n",
        "\\sum_t \\left(\\frac{ \\partial Loss_t }{ \\partial y_t } \\cdot \\frac{ \\partial y_t }{ \\partial W_{hy}} \\right)= \\sum_t( p_t - 1_{target}).h_t^T $$\n",
        "$$ \\frac {\\partial Loss_t }{\\partial b_y} = \\sum_t(p_t - 1_{target}) $$\n",
        "```python\n",
        "dWhy += np.dot(dy, hs[t].T)\n",
        "dby += dy\n",
        "```\n",
        "\n",
        "3. **Gradient w.r.t Hidden State (`h_t`):**\n",
        "To backpropagate into the hidden state, we need to account for both the current time step's gradient and the incoming gradient from the next time step:\n",
        "$$ \\frac{ \\partial Loss_t }{\\partial h_t} = W_{hy}^T \\cdot \\frac{\\partial Loss_t}{ \\partial y_t} + \\frac{\\partial Loss_{t+1}}{\\partial h_t} $$\n",
        "Where:\n",
        "- $ \\frac{\\partial Loss_{t+1}}{\\partial h_t} $ is the gradient passed back from the next time step.\n",
        "```python\n",
        "dh = np.dot(Why.T, dy) + dhnext\n",
        "```\n",
        "\n",
        "4. **Gradient w.r.t Activation Function (`tanh`):**\n",
        "The hidden state is computed using the $ tanh $ activation function:\n",
        "$$ h_t = \\tanh(W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h) $$\n",
        "The input to the `tanh` activation function is:\n",
        "$$ a_t = W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h $$\n",
        "The gradient through the $ tanh $ function is:\n",
        "$$ dhraw = \\frac{\\partial Loss_t}{\\partial a_t} = \\frac{ \\partial Loss_t }{\\partial ({W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h })} = (1 - h_{t}^2) \\odot \\frac{ \\partial Loss_t }{\\partial h_t} $$\n",
        "Here, $ (1-h_t^2) $ is the derivative of $ tanh(h_t) $.\n",
        "```python\n",
        "dhraw = (1- hs[t]*hs[t]) * dh\n",
        "```\n",
        "\n",
        "\n",
        "5. **Gradient w.r.t Input Weights (`Wxh`), Hidden Weights (`Whh`), and Hidden Bias (`bh`):**\n",
        "Now, compute the gradients w.r.t weights and biases connecting the inputs and hidden states:\n",
        "The input to the `tanh` activation function is:\n",
        "$$ a_t = W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h $$\n",
        "- For input-to-hidden weights $ W_{xh} $:\n",
        "$$\n",
        "\\frac{ \\partial \\text{Loss}_t}{\\partial W_{xh}} = \\sum_t \\left( \\frac{ \\partial \\text{Loss}_t}{\\partial a_t} \\cdot \\frac{ \\partial a_t}{\\partial W_{xh}} \\right)= \\sum_t \\left( dhraw \\cdot x_t^T \\right)\n",
        "$$\n",
        "- For hidden-to-hidden weights $ W_{hh} $:\n",
        "$$\n",
        "\\frac{ \\partial \\text{Loss}_t}{\\partial W_{hh}} = \\sum_t \\left(\\frac{ \\partial \\text{Loss}_t}{\\partial a_t} \\cdot \\frac{ \\partial a_t }{\\partial W_{hh}} \\right) = \\sum_t (dhraw \\cdot h_{t-1}^T)\n",
        "$$\n",
        "- For hidden bias $ b_h $:\n",
        "$$\n",
        "\\frac{\\partial Loss_t}{\\partial b_h} = \\sum_t \\frac{\\partial Loss}{\\partial a_t} = \\sum_tdhraw\n",
        "$$\n",
        "```python\n",
        "dbh += dhraw\n",
        "dWxh += np.dot(dhraw, xs[t].T)\n",
        "dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "```\n",
        "\n",
        "6. **Gradient Propagation to Previous Time Step (`dhnext`):**\n",
        "Propogate the gradient back to the previous time step:\n",
        "$$\n",
        "\\frac{ \\partial Loss_t}{\\partial h_{t-1}} = W_{hh}^T \\cdot \\frac{\\partial Loss_t}{\\partial a_t} = W_{hh}^T \\cdot dhraw\n",
        "$$\n",
        "```python\n",
        "dhnext = np.dot(Whh.T, dhraw)\n",
        "```\n",
        "\n",
        "##### Summary:\n",
        "- **Step 1:** Compute gradient of the loss w.r.t the output probabilities.\n",
        "- **Step 2:** Calculate the gradients for the weights and biases connecting hidden states to outputs.\n",
        "- **Step 3:** Propagate the gradient through the hidden state, taking into account the contribution from the next time step.\n",
        "- **Step 4:** Backpropagate through the $ tanh $ activation function.\n",
        "- **Step 5:** Compute the gradients w.r.t the weights and biases connecting the inputs and hidden states, as well as the hidden-to-hidden weights.\n",
        "- **Step 6:** Propagate the gradient back to the previous time step's hidden state.\n",
        "\n",
        "These steps iteratively update the gradient accumulations by iterating backward through the time steps of the sequene."
      ],
      "metadata": {
        "id": "H-iReoyI6_Yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lossFun(inputs, targets, hprev):\n",
        "  \"\"\"\n",
        "  Runs forward and backward passes through the RNN.\n",
        "\n",
        "  inputs, targets: Lists of integers. For some i, inputs[i] is the input\n",
        "                   character (encoded as an index to the ix_to_char map) and\n",
        "                   targets[i] is the corresponding next character in the\n",
        "                   training data (similarly encoded).\n",
        "  hprev: Hx1 array of initial hidden state.\n",
        "  returns: loss, gradients on model parameters, and last hidden state.\n",
        "  \"\"\"\n",
        "  # Caches that keep values computed in the forward pass at each time step,\n",
        "  # to be reused in the backward pass.\n",
        "  xs, hs, ys, ps = {}, {}, {}, {}\n",
        "\n",
        "  # Initial incoming state\n",
        "  hs[-1] = np.copy(hprev)\n",
        "  loss = 0\n",
        "\n",
        "  # Forward pass\n",
        "  for t in range(len(inputs)):\n",
        "    # Input at time step t is xs[t].  Prepare a one-hot encoded vector of shape\n",
        "    # (V, 1). inputs[t] is the index where the 1 goes.\n",
        "    xs[t] = np.zeros((vocab_size, 1))       # encode 1-of-k representation\n",
        "    xs[t][inputs[t]] = 1\n",
        "\n",
        "    # Compute h[t] from h[t-1] and x[t]\n",
        "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)\n",
        "\n",
        "    # Compute ps[t] - softmax probabilities for output\n",
        "    ys[t] = np.dot(Why, hs[t]) + by\n",
        "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
        "\n",
        "  loss += -np.log(ps[t][targets[t], 0])\n",
        "\n",
        "  # Backward pass: compute gradients going backwards\n",
        "  # Gradients are initialized to 0s, and every time step contributes to them\n",
        "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "\n",
        "  # Initialize the incoming gradient of h to zero; this is a safe assumption for\n",
        "  # a sufficiently long unrolling.\n",
        "  dhnext = np.zeros_like(hs[0])\n",
        "\n",
        "  # The backwards pass iterates over the input sequence backwards.\n",
        "  for t in reversed(range(len(inputs))):\n",
        "    # Backprop through the gradients of loss and softmax\n",
        "    dy = np.copy(ps[t])\n",
        "    dy[targets[t]] -= 1\n",
        "\n",
        "    # Compute gradients for the Why and by parameters\n",
        "    dWhy += np.dot(dy, hs[t].T)\n",
        "    dby += dy\n",
        "\n",
        "    # Backprop through the fully-connected layer (Why, by) to h. Also add up the\n",
        "    # incoming gradient for h from the next cell.\n",
        "    # Note: proper Jacobian matmul here would be dy.dot(Why), that would give\n",
        "    # a [1,T] vector. Since we need [T,1] for h, we flip the dot (we could have\n",
        "    # transposed after everything, too)\n",
        "    dh = np.dot(Why.T, dy) + dhnext\n",
        "\n",
        "    # Backprop through tanh\n",
        "    dhraw = (1 - hs[t]*hs[t]) * dh\n",
        "\n",
        "    # Compute gradients for the dby, dWxh, Whh parameters.\n",
        "    dbh += dhraw\n",
        "    dWxh += np.dot(dhraw, xs[t].T)\n",
        "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "\n",
        "    # Backprop the gradient to the incoming h, which will be used in the previous time step.\n",
        "    dhnext = np.dot(Whh.T, dhraw)\n",
        "\n",
        "  # Gradient clipping to the range [-5, 5]\n",
        "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "    np.clip(dparam, -5, 5, out = dparam)\n",
        "\n",
        "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
      ],
      "metadata": {
        "id": "jK41ku0pfIbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(h, seed_ix, n):\n",
        "  \"\"\"\n",
        "  Sample a sequence of integeres from the model.\n",
        "\n",
        "  Runs the RNN in forward mode for n steps;\n",
        "  seed_ix --> The seed letter for the first time step,\n",
        "  h --> The memory state.\n",
        "\n",
        "  Returns a sequence of letters produced by the model (indices).\n",
        "  \"\"\"\n",
        "  # Create a one-hot vector to represent the input.\n",
        "  x = np.zeros((vocab_size, 1))\n",
        "  x[seed_ix] = 1\n",
        "\n",
        "  ixes = []    # Initializes an empty list ixes to store the generated character indices\n",
        "\n",
        "  for t in range(n):\n",
        "    # Run the forward pass only.\n",
        "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
        "    y = np.dot(Why, h) + by\n",
        "    p = np.exp(y) / np.sum(np.exp(y))\n",
        "\n",
        "    # Sample from the distribution produced by softmax.\n",
        "    ix = np.random.choice(range(vocab_size), p = p.ravel())\n",
        "\n",
        "    # Prepare input for the next call\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    x[ix] = 1\n",
        "    ixes.append(ix)\n",
        "  return ixes"
      ],
      "metadata": {
        "id": "_2GqBjf8leIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient checking\n",
        "from random import uniform\n",
        "\n",
        "def gradCheck(inputs, targets, hprev):\n",
        "  \"\"\"\n",
        "  Performs gradient checking to verify that the analytical gradients computed by backpropagation\n",
        "  match the numerical gradients approximated by finite differences.\n",
        "  \"\"\"\n",
        "\n",
        "  global Wxh, Whh, Why, bh, by   # Declare global variables for the RNN parameters.\n",
        "\n",
        "  # Set the number of checks and the small value for numerical gradient calculation.\n",
        "  num_checks, delta = 10, 10e-5\n",
        "\n",
        "  # Compute the analytical gradients using the backpropagation function 'lossFun'\n",
        "  _, dWxh, dWhh, dWhy, dbh, dby, _ = lossFun(inputs, targets, hprev)\n",
        "  for param, dparam, name in zip([Wxh, Whh, Why, bh, by],\n",
        "                                 [dWxh, dWhh, dWhy, dbh, dby],\n",
        "                                 ['Wxh', 'Whh', 'Why', 'bh', 'by']):\n",
        "    # Verify that the dimensions of the parameter and its gradient match.\n",
        "    s0 = dparam.shape\n",
        "    s1 = param.shape\n",
        "    assert s0 == s1, 'Error dims dont match: %s and %s.' % (s0, s1)\n",
        "    print(name)\n",
        "\n",
        "    for i in range(num_checks):\n",
        "\n",
        "      # randomly select an index 'ri' in the flattened parameter array to check.\n",
        "      ri = int(uniform(0, param.size))\n",
        "      # evaluate cost at [x + delta] and [x - delta]\n",
        "\n",
        "      old_val = param.flat[ri]\n",
        "\n",
        "      # compute loss with a positive perturbation at index 'ri'\n",
        "      param.flat[ri] = old_val + delta\n",
        "      cg0, _, _, _, _, _, _ = lossFun(inputs, targets, hprev)  # cg0 is the cost with positive perturbation\n",
        "\n",
        "      # compute loss with a negative perturbation at index 'ri'\n",
        "      param.flat[ri] = old_val - delta\n",
        "      cg1, _, _, _, _, _, _ = lossFun(inputs, targets, hprev)  # cg1 is the cost with negative perturbation\n",
        "\n",
        "      param.flat[ri] = old_val    # reset old value for this parameter\n",
        "\n",
        "\n",
        "      grad_analytic = dparam.flat[ri]   # analytical gradient\n",
        "\n",
        "      grad_numerical = (cg0 - cg1) / (2 * delta) # numerical gradient\n",
        "\n",
        "      rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + grad_analytic)\n",
        "\n",
        "      print('%f, %f => %e ' % (grad_numerical, grad_analytic, rel_error))\n",
        "      # rel_error should be on order of 10e-7 or less\n"
      ],
      "metadata": {
        "id": "DVDTIbV-52lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function invokes gradCheck with all the parameters properly set up.\n",
        "def basicGradCheck():\n",
        "  inputs = [char_to_ix[ch] for ch in data[:seq_length]]\n",
        "  targets = [char_to_ix[ch] for ch in data[1:seq_length + 1]]\n",
        "  hprev = np.zeros((hidden_size, 1))      # reset RNN memory\n",
        "  gradCheck(inputs, targets, hprev)"
      ],
      "metadata": {
        "id": "dllrB4w_-Y3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basicGradCheck()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWl9xCCd_CVD",
        "outputId": "36e2c4a7-c904-4961-c54d-7fd04ff5a6b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wxh\n",
            "0.000000, 0.000000 => nan \n",
            "0.000000, 0.000000 => nan \n",
            "0.000022, -0.010993 => 1.004099e+00 \n",
            "0.000000, 0.000000 => nan \n",
            "0.000000, 0.000000 => nan \n",
            "0.000000, 0.000000 => nan \n",
            "0.000000, 0.000000 => nan \n",
            "0.000000, 0.000000 => nan \n",
            "-0.000000, 0.002755 => 1.000000e+00 \n",
            "0.000000, 0.000000 => nan \n",
            "Whh\n",
            "0.000286, -0.000009 => 1.064862e+00 \n",
            "0.000037, -0.000186 => 1.498958e+00 \n",
            "-0.000175, -0.000089 => 3.236123e-01 \n",
            "0.000051, -0.000144 => 2.089605e+00 \n",
            "-0.000001, -0.000248 => 9.942125e-01 \n",
            "-0.000019, -0.000058 => 5.032077e-01 \n",
            "0.000070, 0.000087 => 1.114804e-01 \n",
            "0.000246, 0.000126 => 3.212826e-01 \n",
            "-0.000015, 0.000427 => 1.072026e+00 \n",
            "0.000098, 0.000241 => 4.226525e-01 \n",
            "Why\n",
            "0.000139, -0.000088 => 4.379527e+00 \n",
            "0.000055, 0.000650 => 8.441306e-01 \n",
            "-0.000169, -0.000024 => 7.513294e-01 \n",
            "-0.000256, 0.001042 => 1.651875e+00 \n",
            "-0.000037, -0.000500 => 8.625015e-01 \n",
            "-0.000194, -0.000241 => 1.080471e-01 \n",
            "-0.000030, 0.007120 => 1.008393e+00 \n",
            "-0.000024, -0.000072 => 5.043514e-01 \n",
            "-0.000022, 0.000276 => 1.172936e+00 \n",
            "-0.000158, -0.000004 => 9.480913e-01 \n",
            "bh\n",
            "-0.011852, -0.053949 => 6.397735e-01 \n",
            "-0.013250, 0.012329 => 2.777415e+01 \n",
            "0.007895, 0.054932 => 7.486721e-01 \n",
            "0.006380, 0.016476 => 4.417011e-01 \n",
            "-0.011719, 0.028062 => 2.434106e+00 \n",
            "0.005605, 0.092616 => 8.858622e-01 \n",
            "0.022458, 0.043480 => 3.188146e-01 \n",
            "0.007753, -0.033593 => 1.600089e+00 \n",
            "0.006380, 0.016476 => 4.417011e-01 \n",
            "0.022458, 0.043480 => 3.188146e-01 \n",
            "by\n",
            "0.015379, 0.246139 => 8.823899e-01 \n",
            "0.015402, 0.246181 => 8.822377e-01 \n",
            "0.015382, 0.246166 => 8.823776e-01 \n",
            "0.015370, 0.246114 => 8.824374e-01 \n",
            "0.015383, 0.246177 => 8.823775e-01 \n",
            "0.015377, 0.246113 => 8.823894e-01 \n",
            "0.015409, 0.246269 => 8.822285e-01 \n",
            "0.015373, 0.246139 => 8.824268e-01 \n",
            "0.015366, 0.246199 => 8.825041e-01 \n",
            "0.015376, 0.246174 => 8.824246e-01 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n is the iteration counter; p is the sequence pointer, at the beginning\n",
        "# of each step it points at the sequence in the input that will be used for\n",
        "# training this iteration.\n",
        "n, p = 0, 0\n",
        "\n",
        "# Memory variables for Adagrad\n",
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by)\n",
        "smooth_loss = -np.log(1.0 / vocab_size)*seq_length\n",
        "\n",
        "while p < MAX_DATA:\n",
        "  # Preprare inputs (we're sweeping from left to right in steps seq_length long)\n",
        "  if (p+seq_length+1) >= len(data) or n == 0:\n",
        "    hprev = np.zeros((hidden_size, 1))      # reset RNN memory\n",
        "    p = 0                                   # go from start of data\n",
        "\n",
        "  # In each step we unroll the RNN for seq_length cells, and present it with\n",
        "  # seq_length inputs and seq_length target outputs to learn.\n",
        "\n",
        "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
        "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
        "\n",
        "  # Sample from the model now and then\n",
        "  if n % 1000 == 0:\n",
        "    sample_ix = sample(hprev, inputs[0], 200)\n",
        "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
        "    print('----\\n %s \\n----' % (txt, ))\n",
        "\n",
        "  # Forward seq_length characters through the net and fetch gradient\n",
        "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "  if n % 200 == 0: print('iter %d (p=%d), loss: %f' % (n, p, smooth_loss))\n",
        "\n",
        "  # Perform parameter update with Adagrad\n",
        "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
        "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
        "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "    mem += dparam * dparam\n",
        "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
        "\n",
        "  p += seq_length\n",
        "  n += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imTrnyA-_EQS",
        "outputId": "af1f2e15-009d-48bf-b74a-d0b6f6eaca92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----\n",
            " jItHb\n",
            "&&P:iwF-;o?yJIcbfmtr,;-':FQktiUYyjc UqqLLaOI&vWL3cX;upkDYXcT-Xo'jTMISLSkSKuxFirUUUDm?EX-rENrubNr,ihp$tQrUE;'IGkjcDDbuvI;TP:mNK!KoUEJlU-pHGdk'Fku3X-GTE.,QuIzLjIBjnuhu?UiSZAWGE:.Upfgv,j&BT$uCg;:pB \n",
            "----\n",
            "iter 0 (p=0), loss: 66.727580\n",
            "iter 200 (p=3200), loss: 55.333248\n",
            "iter 400 (p=6400), loss: 45.886402\n",
            "iter 600 (p=9600), loss: 38.130851\n",
            "iter 800 (p=12800), loss: 31.752094\n",
            "----\n",
            " pir t hor ony: l:is ma\n",
            "Toud h aondet\n",
            "Vieus orl tom.\n",
            "L ongf antassulos Ve.y,,\n",
            "\n",
            "VpUIe I\n",
            "sUGo de 3y mhh: the thiw Led oh henon aeuwsaens\n",
            "V a de\n",
            "\n",
            "OrGSg ctayau lu fwin,, s auu di thetrhonbaai atasenuesalIl \n",
            "----\n",
            "iter 1000 (p=16000), loss: 26.498397\n",
            "iter 1200 (p=19200), loss: 22.180755\n",
            "iter 1400 (p=22400), loss: 18.639481\n",
            "iter 1600 (p=25600), loss: 15.764989\n",
            "iter 1800 (p=28800), loss: 13.387624\n",
            "----\n",
            " n baseu nhphetthho wcisbe lnithen, tathetel,\n",
            "Wot darsisi waras olp desbea apeth nut'er harikathvz shaceese Moil orcon gonudheo mow\n",
            "Aesu ne vebt thegen bid on me odpl bes-arelles kayyo., \n",
            "\n",
            "Mgul yoldeI  \n",
            "----\n",
            "iter 2000 (p=32000), loss: 11.430672\n",
            "iter 2200 (p=35200), loss: 9.844516\n",
            "iter 2400 (p=38400), loss: 8.479590\n",
            "iter 2600 (p=41600), loss: 7.420036\n",
            "iter 2800 (p=44800), loss: 6.558909\n",
            "----\n",
            " f whenthanasetim fiesud mly barar ton,\n",
            "As:\n",
            "Cin fllleltt\n",
            " or hes ir f,\n",
            "Tives are.\n",
            "Jargind\n",
            "\n",
            "US:\n",
            "He famte,\n",
            "F:\n",
            "Fivenrn\n",
            "\n",
            "Cend Vavere pan;\n",
            " wpwalfein re\n",
            "yeet thyses,i ff-ca,t nindlei; fot nef theer as arege \n",
            "----\n",
            "iter 3000 (p=48000), loss: 5.804955\n",
            "iter 3200 (p=51200), loss: 5.199258\n",
            "iter 3400 (p=54400), loss: 4.673734\n",
            "iter 3600 (p=57600), loss: 4.252711\n",
            "iter 3800 (p=60800), loss: 3.875203\n",
            "----\n",
            " e l findiiueso\n",
            "\n",
            "SINI:\n",
            "CS:\n",
            "GI lack hot a katkees you lii wive wes wre and housind tim yoully thind sing doen;\n",
            "Hor\n",
            "Inltd have tou you ti\n",
            ".\n",
            " Rand hlenlchiuds me\n",
            ":\n",
            "CArhou co godint ait,\n",
            "veand. tiy Co aod  \n",
            "----\n",
            "iter 4000 (p=64000), loss: 3.590685\n",
            "iter 4200 (p=67200), loss: 3.330068\n",
            "iter 4400 (p=70400), loss: 3.116095\n",
            "iter 4600 (p=73600), loss: 3.003922\n",
            "iter 4800 (p=76800), loss: 2.860703\n",
            "----\n",
            " :; cure!\n",
            "\n",
            "CENIUS:\n",
            "WhiNs:\n",
            "Throur go cpwere!\n",
            "co;\n",
            "Aso:\n",
            ":\n",
            "INIhe so wheqa;\n",
            "Weir tonchy spusetor'ety\n",
            "Asgh: rathe, fout'at pere an.\n",
            "Himpoh recearc yrifhat\n",
            "Af\n",
            "Lhe hf wat shith the plasen:\n",
            "Thellis\n",
            "Oe thet uw s \n",
            "----\n",
            "iter 5000 (p=80000), loss: 2.749573\n",
            "iter 5200 (p=83200), loss: 2.681586\n",
            "iter 5400 (p=86400), loss: 2.593996\n",
            "iter 5600 (p=89600), loss: 2.548068\n",
            "iter 5800 (p=92800), loss: 2.509304\n",
            "----\n",
            " -gthy tallt veer whe, womiso:\n",
            "I mcow he y mame ond thee nopith be suth bue:\n",
            "I hind\n",
            "Ler me thoak hof rateles, heass ghes oris.\n",
            "\n",
            "VOMEN:\n",
            ":\n",
            "IF thiretpot's, lotta preort foor. way moupr hipithit ie rinttci \n",
            "----\n",
            "iter 6000 (p=96000), loss: 2.498152\n",
            "iter 6200 (p=99200), loss: 2.445430\n",
            "iter 6400 (p=102400), loss: 2.423344\n",
            "iter 6600 (p=105600), loss: 2.426720\n",
            "iter 6800 (p=108800), loss: 2.351471\n",
            "----\n",
            "  nem dort hipainghy bick tontle? wit of, yel jud temendis inee ar ppyoll thure yore's ha,h ard:\n",
            "Yor, nicd tave pave,\n",
            "I Roor ob thoun mate mome arer wale weistone .\n",
            "\n",
            "A; bbyof thand and min dot; welW, m \n",
            "----\n",
            "iter 7000 (p=112000), loss: 2.330849\n",
            "iter 7200 (p=115200), loss: 2.316997\n",
            "iter 7400 (p=118400), loss: 2.323611\n",
            "iter 7600 (p=121600), loss: 2.330150\n",
            "iter 7800 (p=124800), loss: 2.309742\n",
            "----\n",
            " \n",
            "Gatt tolendire wo, tove fale\n",
            "de sfit warems ther diow from ard airgy as hase cot I thagf nont, wave agh there ways y bay st!\n",
            "Mhid fom nir ato ne?magt,\n",
            "To and fad!\n",
            "\n",
            "ORUS:\n",
            "With an:\n",
            "Isr Rriantelle eomes \n",
            "----\n",
            "iter 8000 (p=128000), loss: 2.303861\n",
            "iter 8200 (p=131200), loss: 2.313763\n",
            "iter 8400 (p=134400), loss: 2.311456\n",
            "iter 8600 (p=137600), loss: 2.318809\n",
            "iter 8800 (p=140800), loss: 2.336469\n",
            "----\n",
            " r as on! i thiug poke wame dend and sere so hik ued, Thile\n",
            "Beeat don's, bee.\n",
            "I cood.\n",
            "\n",
            "MENENIUS:\n",
            "Math turco ane, Sonto Ro hate boktet, ond Sere, I bans\n",
            " Vit winhrase scat buct yo why moud.\n",
            "\n",
            "Sland ir, a \n",
            "----\n",
            "iter 9000 (p=144000), loss: 2.303787\n",
            "iter 9200 (p=147200), loss: 2.313789\n",
            "iter 9400 (p=150400), loss: 2.300829\n",
            "iter 9600 (p=153600), loss: 2.301333\n",
            "iter 9800 (p=156800), loss: 2.298038\n",
            "----\n",
            " wall, ard, marerel ualle, yowess,\n",
            "Ais srreracabronone sowwI. nowhIt hich ind Ronlibg.\n",
            "\n",
            "CAA:\n",
            "Ind\n",
            "Tway'd coun mor ken mimoprOpall fare, dolshy dis ant thes, erllland\n",
            "Foucere his ware lyis ond tole thes\n",
            " \n",
            "----\n",
            "iter 10000 (p=160000), loss: 2.307835\n",
            "iter 10200 (p=163200), loss: 2.286955\n",
            "iter 10400 (p=166400), loss: 2.287206\n",
            "iter 10600 (p=169600), loss: 2.301995\n",
            "iter 10800 (p=172800), loss: 2.277514\n",
            "----\n",
            " ou me wy his'd,\n",
            "He ciths;,\n",
            "Hid make Hesuce, sot hecbes houde wady euch mrspet con debthe musgsne penill.\n",
            "\n",
            "GLARUS:\n",
            "Her af acafd doy I hy!\n",
            "\n",
            "LANA:\n",
            "BG\n",
            "Ynthe ur soonet ild gou snwhoti,\n",
            "And fuper that and g \n",
            "----\n",
            "iter 11000 (p=176000), loss: 2.335501\n",
            "iter 11200 (p=179200), loss: 2.318797\n",
            "iter 11400 (p=182400), loss: 2.308180\n",
            "iter 11600 (p=185600), loss: 2.282763\n",
            "iter 11800 (p=188800), loss: 2.222489\n",
            "----\n",
            " horscsthe ce bume y eingetem, srler fostate sny un-Hiy lont,\n",
            "Withe be tereinnwit for in I in thetmar oflen, pard mengt reane ceks, pngofen sjathseat cfoonthard.\n",
            "\n",
            "Lhe hiy es nvienet wose,\n",
            "I, sey co seg \n",
            "----\n",
            "iter 12000 (p=192000), loss: 2.234706\n",
            "iter 12200 (p=195200), loss: 2.223377\n",
            "iter 12400 (p=198400), loss: 2.195463\n",
            "iter 12600 (p=201600), loss: 2.159567\n",
            "iter 12800 (p=204800), loss: 2.206349\n",
            "----\n",
            " or inm notd I aed heags andenesj\n",
            "Otmm mat deavs oo in dow matf youel yek\n",
            "Digh't dril waw:\n",
            "Henthy whe thanof, so hing,\n",
            "Giched durith subdoand, ce.\n",
            "\n",
            "SowU hiNh condat fot inneev.ar must meigher col hape  \n",
            "----\n",
            "iter 13000 (p=208000), loss: 2.217348\n",
            "iter 13200 (p=211200), loss: 2.236339\n",
            "iter 13400 (p=214400), loss: 2.241534\n",
            "iter 13600 (p=217600), loss: 2.249618\n",
            "iter 13800 (p=220800), loss: 2.273292\n",
            "----\n",
            " h A:\n",
            "Borl to thee mill ie mit jos wine to that the noke laRer, os withs\n",
            "Maw my land in, yiping goum'leker:\n",
            "Pat\n",
            "Artoord you,\n",
            "I the most gmy baganch, be adnte thes\n",
            "Wath Icusk!\n",
            "Dovendid the fras I I sive \n",
            "----\n",
            "iter 14000 (p=224000), loss: 2.256542\n",
            "iter 14200 (p=227200), loss: 2.255522\n",
            "iter 14400 (p=230400), loss: 2.246203\n",
            "iter 14600 (p=233600), loss: 2.231128\n",
            "iter 14800 (p=236800), loss: 2.219346\n",
            "----\n",
            " seaver\n",
            "Bok?\n",
            "\n",
            "SITCESINGE: theet, roor Cacust:\n",
            "To hiCCO; Erjhen thie ser dowindaren?\n",
            "You antcen airad, pe?\n",
            "Whosd ar anet theent ghe ke with Re beed, brereourseld thend'f\n",
            "Sond pron nor woor:\n",
            "To fo; lmeng \n",
            "----\n",
            "iter 15000 (p=240000), loss: 2.204914\n",
            "iter 15200 (p=243200), loss: 2.188457\n",
            "iter 15400 (p=246400), loss: 2.187868\n",
            "iter 15600 (p=249600), loss: 2.198068\n",
            "iter 15800 (p=252800), loss: 2.186122\n",
            "----\n",
            " the or of, so fors in all in nis nakd unt, ul,\n",
            "And ie we.\n",
            "\n",
            "GLORGOUCINIUS:\n",
            "Wheace of mesins the,\n",
            "IUthad Jtyou cithst eis sporle Sesencars theckirc oplelt.\n",
            "\n",
            "CABEHASTOR:\n",
            "I fout,\n",
            "Gnvesallpoce wors' freeny \n",
            "----\n",
            "iter 16000 (p=256000), loss: 2.200765\n",
            "iter 16200 (p=259200), loss: 2.203879\n",
            "iter 16400 (p=262400), loss: 2.202766\n",
            "iter 16600 (p=265600), loss: 2.213139\n",
            "iter 16800 (p=268800), loss: 2.229932\n",
            "----\n",
            " ou; uk'd tore kepe\n",
            "'d om g thend lontee thouring blem is ;\n",
            "Hegant ceade aad oorat sow ke you.\n",
            "Ther gave kis,\n",
            "Agh of my anam:\n",
            "Thy locr, thing is awd ir in nis ond croweld, hreat'd'litted's,\n",
            "Nathifh co. \n",
            "----\n",
            "iter 17000 (p=272000), loss: 2.223152\n",
            "iter 17200 (p=275200), loss: 2.229631\n",
            "iter 17400 (p=278400), loss: 2.216380\n",
            "iter 17600 (p=281600), loss: 2.205609\n",
            "iter 17800 (p=284800), loss: 2.193605\n",
            "----\n",
            " lovetta\n",
            "Enderest your machste\n",
            "Mtadan is hrourt whict I a hak GERHiv:\n",
            "Eo sive entnt enth;\n",
            "Uijnd Os heinel foreet hof pod, by b?\n",
            "wmy ladnes yrued duysor so mecr sor deacroot rey one,\n",
            "RICTDRING RORH:\n",
            "He  \n",
            "----\n",
            "iter 18000 (p=288000), loss: 2.176467\n",
            "iter 18200 (p=291200), loss: 2.125611\n",
            "iter 18400 (p=294400), loss: 2.110203\n",
            "iter 18600 (p=297600), loss: 2.089280\n",
            "iter 18800 (p=300800), loss: 2.121791\n",
            "----\n",
            " ?\n",
            "\n",
            "KINGHAIY III:\n",
            "I baghutirink hou dens terarthurs mim-\n",
            "The nor, hus tingh's loul lore joruk ftorre. foad;\n",
            "No kast ot marm cun liy yourer ther stalon, wher fom of bor hat math\n",
            "ul fe, werur lorl Os the \n",
            "----\n",
            "iter 19000 (p=304000), loss: 2.151626\n",
            "iter 19200 (p=307200), loss: 2.157958\n",
            "iter 19400 (p=310400), loss: 2.181912\n",
            "iter 19600 (p=313600), loss: 2.145492\n",
            "iter 19800 (p=316800), loss: 2.145664\n",
            "----\n",
            " ritpr bory modgs the theie I meerane of,\n",
            ".\n",
            "'ntlf o but oo dpercildad we'd blott and Mfece fovtem,\n",
            "as sue;\n",
            "Codi's bealsredorhen hume ap,\n",
            "Aik me in an, I toram, nows mume,\n",
            "Angse-\n",
            "You the boyr ducirmef i \n",
            "----\n",
            "iter 20000 (p=320000), loss: 2.193088\n",
            "iter 20200 (p=323200), loss: 2.212411\n",
            "iter 20400 (p=326400), loss: 2.234051\n",
            "iter 20600 (p=329600), loss: 2.241791\n",
            "iter 20800 (p=332800), loss: 2.258710\n",
            "----\n",
            " by old tfoul,\n",
            "As wiravamas cry me hing ther pemy,\n",
            "And froor.\n",
            "\n",
            "CKING On Ad hisss shpees jo lotes,\n",
            "And pive Ducolin af Here un Arethes obroth t.\n",
            "And sale thad wing hmat hangher mast' and tors,\n",
            "\n",
            "Leth sst \n",
            "----\n",
            "iter 21000 (p=336000), loss: 2.275889\n",
            "iter 21200 (p=339200), loss: 2.287349\n",
            "iter 21400 (p=342400), loss: 2.258905\n",
            "iter 21600 (p=345600), loss: 2.246078\n",
            "iter 21800 (p=348800), loss: 2.237189\n",
            "----\n",
            " t sha leald wealots, Mat sh.\n",
            "\n",
            "KIN Ewt:\n",
            "Lay loten sorrsens I whoullaralf low reves tige parcom oult wathitn cry sear,\n",
            "Futhut,\n",
            "Whore preall, agf sest inf so Gath; wilm reard nrath.\n",
            "So of wefrt mive by a \n",
            "----\n",
            "iter 22000 (p=352000), loss: 2.188371\n",
            "iter 22200 (p=355200), loss: 2.169713\n",
            "iter 22400 (p=358400), loss: 2.198244\n",
            "iter 22600 (p=361600), loss: 2.239028\n",
            "iter 22800 (p=364800), loss: 2.222069\n",
            "----\n",
            " em,\n",
            "Whis sor deat in andt\n",
            "Cot'tr. you That Storim.\n",
            "\n",
            "BARD:\n",
            "Wo in this mumcsslef be But cham suounes,\n",
            "Whou nold pase as of hichorpunf mesein hiveece os uksurs Warl\n",
            "I he cfy diis ist-Care ame wente,\n",
            "Celi \n",
            "----\n",
            "iter 23000 (p=368000), loss: 2.218608\n",
            "iter 23200 (p=371200), loss: 2.148127\n",
            "iter 23400 (p=374400), loss: 2.179700\n",
            "iter 23600 (p=377600), loss: 2.213271\n",
            "iter 23800 (p=380800), loss: 2.188362\n",
            "----\n",
            " insex dois engce kingw heredere ay wour sontm\n",
            "At Bomes freenths ke selinc relding wamen grooras'.\n",
            "Matits tor whou wel hit so whe your wean slat orchars uistoob\n",
            "Seabes, but the daystours with stiend wh \n",
            "----\n",
            "iter 24000 (p=384000), loss: 2.180034\n",
            "iter 24200 (p=387200), loss: 2.176849\n",
            "iter 24400 (p=390400), loss: 2.157722\n",
            "iter 24600 (p=393600), loss: 2.178459\n",
            "iter 24800 (p=396800), loss: 2.156812\n",
            "----\n",
            " is he his and\n",
            "A. le\n",
            "Wof llagharsny cord,\n",
            "Sars.\n",
            "Monibr:\n",
            "Whathoke whest, were with stend? then bar'm prears her, hive.\n",
            "\n",
            "DR:\n",
            "Self?\n",
            "\n",
            "GoRH.\n",
            "briexes,\n",
            "Formeith thou costy bot.\n",
            "\n",
            "IT:\n",
            "SBow pat?\n",
            "\n",
            "Fouss jonter ta \n",
            "----\n",
            "iter 25000 (p=400000), loss: 2.192570\n",
            "iter 25200 (p=403200), loss: 2.181669\n",
            "iter 25400 (p=406400), loss: 2.173080\n",
            "iter 25600 (p=409600), loss: 2.185084\n",
            "iter 25800 (p=412800), loss: 2.139136\n",
            "----\n",
            " um no moullle in detkered.\n",
            "\n",
            "THROUC:\n",
            "Letell'd\n",
            "You I by thiep hatp younsf cingl are\n",
            "I sitha me sereses boce sercnor tha hit share thoug arte bare this liy?\n",
            "\n",
            "KING RIMIBROUKE:\n",
            "Grignat woriet is that and,\n",
            " \n",
            "----\n",
            "iter 26000 (p=416000), loss: 2.124674\n",
            "iter 26200 (p=419200), loss: 2.112061\n",
            "iter 26400 (p=422400), loss: 2.080380\n",
            "iter 26600 (p=425600), loss: 2.079103\n",
            "iter 26800 (p=428800), loss: 2.063303\n",
            "----\n",
            " RY YORK:\n",
            "Thone brine hantou womther sh ples mucces shout by 'prost sandele sprece, harkented brAreow mase grorend my book tind ta\n",
            "O Yokilshun a dem pibeadt shat of of of ha handle mape din our;\n",
            "Sas an \n",
            "----\n",
            "iter 27000 (p=432000), loss: 2.040474\n",
            "iter 27200 (p=435200), loss: 2.037590\n",
            "iter 27400 (p=438400), loss: 2.030763\n",
            "iter 27600 (p=441600), loss: 2.031476\n",
            "iter 27800 (p=444800), loss: 2.052133\n",
            "----\n",
            " y, eking\n",
            "Nwane,\n",
            "Habl, port suty whearsow if Ithyes,\n",
            "I that care I hehdies I witize kroww:\n",
            "Hath bongemeene groueghie was,y shour now nowhaln loallinf shtt a cet my stor thes the:\n",
            "Hpalle, pparint; croug \n",
            "----\n",
            "iter 28000 (p=448000), loss: 2.068525\n",
            "iter 28200 (p=451200), loss: 2.066129\n",
            "iter 28400 (p=454400), loss: 2.063726\n",
            "iter 28600 (p=457600), loss: 2.099903\n",
            "iter 28800 (p=460800), loss: 2.124241\n",
            "----\n",
            " \n",
            "\n",
            "HES:\n",
            "Cith tave nat love;\n",
            "The pactst,\n",
            "Whe!\n",
            "Bums mongord, nrall eof leblems hime I proaven the blame, goup, brewouce wide sawen cat thy balle ard sould sour.\n",
            "\n",
            "AK:\n",
            "Will paill shal tarstreed coud she,\n",
            "A \n",
            "----\n",
            "iter 29000 (p=464000), loss: 2.163800\n",
            "iter 29200 (p=467200), loss: 2.186824\n",
            "iter 29400 (p=470400), loss: 2.161330\n",
            "iter 29600 (p=473600), loss: 2.114160\n",
            "iter 29800 (p=476800), loss: 2.139094\n",
            "----\n",
            " ad and have sar finitn freis, mom; bose, kaus t fimy, what the mont in thit Co hall. in makter hartet\n",
            "ar ant thon.\n",
            "Whith dime, torr beth trich Yo the wo than ar watl groikn,\n",
            "To the larl stald the sull \n",
            "----\n",
            "iter 30000 (p=480000), loss: 2.165910\n",
            "iter 30200 (p=483200), loss: 2.130904\n",
            "iter 30400 (p=486400), loss: 2.089164\n",
            "iter 30600 (p=489600), loss: 2.073140\n",
            "iter 30800 (p=492800), loss: 2.103690\n",
            "----\n",
            " o wive pory woccosgen to hand be pe\n",
            "RAch you aur ding to be;\n",
            "Nou be faverave pave.\n",
            "\n",
            "NVOT:\n",
            "Ind frayble dor aseral se ionieg pane a be to thith. o lay.\n",
            "\n",
            "MoRlolfom thit must, in ear sask the hille mike-l \n",
            "----\n",
            "iter 31000 (p=496000), loss: 2.118306\n",
            "iter 31200 (p=499200), loss: 2.132623\n",
            "iter 31400 (p=502400), loss: 2.149466\n",
            "iter 31600 (p=505600), loss: 2.137733\n",
            "iter 31800 (p=508800), loss: 2.144649\n",
            "----\n",
            " st,-''\n",
            "\n",
            "RERBOMEO:\n",
            "I com! Ricin and o Boue!\n",
            "\n",
            "ROLIO: ad ferden, mow?\n",
            "\n",
            "JOLII:\n",
            "Chas; lay.\n",
            "\n",
            "RERT'd\n",
            "O nreitind, darthere\n",
            "Jo no domee, hild dent you buers,\n",
            "\n",
            "NRidhis a knwevon wiust bughs fursed\n",
            "Thave deante! \n",
            "----\n",
            "iter 32000 (p=512000), loss: 2.140233\n",
            "iter 32200 (p=515200), loss: 2.146319\n",
            "iter 32400 (p=518400), loss: 2.166466\n",
            "iter 32600 (p=521600), loss: 2.179853\n",
            "iter 32800 (p=524800), loss: 2.202162\n",
            "----\n",
            " ivs or ckely sul wath tillorincCha dorp staws will\n",
            "Wouree!\n",
            "\n",
            "Four!\n",
            "\n",
            "QrET:\n",
            "Hourty hime me lod, have not and anf,\n",
            "MEvo yhmen and malins comen wape dis, ise terthefrry for care des, breof mater omed thy f \n",
            "----\n",
            "iter 33000 (p=528000), loss: 2.205850\n",
            "iter 33200 (p=531200), loss: 2.167034\n",
            "iter 33400 (p=534400), loss: 2.141519\n",
            "iter 33600 (p=537600), loss: 2.123675\n",
            "iter 33800 (p=540800), loss: 2.103820\n",
            "----\n",
            "  lim cove.\n",
            "\n",
            "AULIET:\n",
            "And,- colls;\n",
            "Bos ind be veardene shaun.\n",
            "\n",
            "JUPII:\n",
            "\n",
            "JRARTY: food ment rontist? mes, nethed;\n",
            "Aimare Por hiate,\n",
            "Ack and bemij hname tore mheve's marter nstor-heae are; not yolep vetongu \n",
            "----\n",
            "iter 34000 (p=544000), loss: 2.097578\n",
            "iter 34200 (p=547200), loss: 2.091588\n",
            "iter 34400 (p=550400), loss: 2.066998\n",
            "iter 34600 (p=553600), loss: 2.068367\n",
            "iter 34800 (p=556800), loss: 2.118644\n",
            "----\n",
            " nd shand wit.\n",
            "\n",
            "MES LON:\n",
            "Land wich nrterion not for lovice and mevim the pomys thisp:\n",
            "Meinhings: hesh-sintws, be tourg. fy torey you the loretiff'd nelive the denk.\n",
            "But in tith's your,\n",
            "And and hon the  \n",
            "----\n",
            "iter 35000 (p=560000), loss: 2.130437\n",
            "iter 35200 (p=563200), loss: 2.119622\n",
            "iter 35400 (p=566400), loss: 2.118491\n",
            "iter 35600 (p=569600), loss: 2.101660\n",
            "iter 35800 (p=572800), loss: 2.112386\n",
            "----\n",
            " you manolt comest I hies\n",
            "Meat,\n",
            "CAPh that my breat me me yout,\n",
            "And wom hime the mister you theie, I gorselil,-binet?\n",
            "Theted..\n",
            "Samead thenpur in thy er tond, wol siir thou the wark, meand whim: wall the \n",
            "----\n",
            "iter 36000 (p=576000), loss: 2.069807\n",
            "iter 36200 (p=579200), loss: 2.065781\n",
            "iter 36400 (p=582400), loss: 2.073994\n",
            "iter 36600 (p=585600), loss: 2.072407\n",
            "iter 36800 (p=588800), loss: 2.064797\n",
            "----\n",
            " rger veosmy, hat altfor fae; this well on wore.\n",
            "\n",
            "NxULCHEGENNRREND:\n",
            "An.\n",
            "\n",
            "KIEG HARD:\n",
            "Ezist, tha;\n",
            "Sade.\n",
            "\n",
            "EOMEGHA:\n",
            "Hace, thyellell to anver dou therd-swencrour gare,\n",
            "Ed\n",
            "Lerearery rome;\n",
            "Wtir, amd nou wit t \n",
            "----\n",
            "iter 37000 (p=592000), loss: 2.067399\n",
            "iter 37200 (p=595200), loss: 2.061195\n",
            "iter 37400 (p=598400), loss: 2.059535\n",
            "iter 37600 (p=601600), loss: 2.074088\n",
            "iter 37800 (p=604800), loss: 2.090533\n",
            "----\n",
            " adis ugh theot? I Hent grincon;\n",
            "Ind! I weis of chem the Le:\n",
            "live hive the stetcarw.\n",
            "\n",
            "ERICK:\n",
            "This dramthy a whom's wer in in,\n",
            "No uping thele, the wach ang shell cofald fust\n",
            "and. Ant nakn!\n",
            "Ry thyre cium \n",
            "----\n",
            "iter 38000 (p=608000), loss: 2.084802\n",
            "iter 38200 (p=611200), loss: 2.082114\n",
            "iter 38400 (p=614400), loss: 2.100752\n",
            "iter 38600 (p=617600), loss: 2.121360\n",
            "iter 38800 (p=620800), loss: 2.101212\n",
            "----\n",
            "  ctier yout bug theny wow bors she tupaveabl of sraller will ming's Mordsyy rake stay'.\n",
            "\n",
            "GUENNGEwe,\n",
            "Ands ic, iur unthes\n",
            "tutugscas stowhss me.\n",
            "\n",
            "QUET:\n",
            "Ther\n",
            "He?\n",
            "\n",
            "Dmenite busw;\n",
            "Whentt the wat thy thas Ie  \n",
            "----\n",
            "iter 39000 (p=624000), loss: 2.096263\n",
            "iter 39200 (p=627200), loss: 2.087880\n",
            "iter 39400 (p=630400), loss: 2.094204\n",
            "iter 39600 (p=633600), loss: 2.138494\n",
            "iter 39800 (p=636800), loss: 2.127406\n",
            "----\n",
            " kered, hink a right reatsy,\n",
            "Yind so loke a be wang ut fhe in now\n",
            "So bips wove's whe wiys geclev's:\n",
            "Hafr?\n",
            "\n",
            "ELAMd sith I ureord benee stond me thigha, for mus that you hes and's hear mom, you eins, with \n",
            "----\n",
            "iter 40000 (p=640000), loss: 2.106149\n",
            "iter 40200 (p=643200), loss: 2.079602\n",
            "iter 40400 (p=646400), loss: 2.038051\n",
            "iter 40600 (p=649600), loss: 2.061223\n",
            "iter 40800 (p=652800), loss: 2.074607\n",
            "----\n",
            " A the cunguighou ay sur as Fle no hord,\n",
            "Will tiens is nom hates,\n",
            "Seody your manouch;\n",
            "Whape snourd wite be mionel wand be.\n",
            "Bot grouild\n",
            "My?\n",
            "\n",
            "KINY RISKY:\n",
            "3o pimat and bort,\n",
            "Whind? lipeis, verenek and, wo \n",
            "----\n",
            "iter 41000 (p=656000), loss: 2.099949\n",
            "iter 41200 (p=659200), loss: 2.083866\n",
            "iter 41400 (p=662400), loss: 2.090167\n",
            "iter 41600 (p=665600), loss: 2.046382\n",
            "iter 41800 (p=668800), loss: 2.061757\n",
            "----\n",
            " st levey srend whing of and anded,\n",
            "Toud 'is me hope sulv simer this stisnd youschard and\n",
            "WILKING NORBUMEN: I Witowat se, that ar lray'ar he cked wove the Ficce in 'le aI hele I pay hem!\n",
            "That and comve \n",
            "----\n",
            "iter 42000 (p=672000), loss: 2.036731\n",
            "iter 42200 (p=675200), loss: 2.033813\n",
            "iter 42400 (p=678400), loss: 2.014044\n",
            "iter 42600 (p=681600), loss: 2.042530\n",
            "iter 42800 (p=684800), loss: 2.053520\n",
            "----\n",
            " shiat fallow gretenths ofrelf he? ur Gord now whill high pit than g't youe 'ren proling, Ifarl cournet, oule rofagnd.\n",
            "\n",
            "CLARD!:\n",
            "Sod,.\n",
            "\n",
            "MRoRd:'l Kir.\n",
            "\n",
            "KARGORWARGEV:\n",
            "And scis putarstorder efoks, me chard \n",
            "----\n",
            "iter 43000 (p=688000), loss: 2.034720\n",
            "iter 43200 (p=691200), loss: 2.014569\n",
            "iter 43400 (p=694400), loss: 2.025558\n",
            "iter 43600 (p=697600), loss: 2.030939\n",
            "iter 43800 (p=700800), loss: 2.048048\n",
            "----\n",
            " ausest thome a thou to kell wome to heest.\n",
            "\n",
            "KING EN-The but eore a you bnyart aghir mane my want ougmans?\n",
            "\n",
            "SING ELARE:\n",
            "Thinks's ply thy; and I ufarder' stoucory, er.\n",
            "\n",
            "QUCES:\n",
            "Qpe, thinnond matie broumy \n",
            "----\n",
            "iter 44000 (p=704000), loss: 2.081802\n",
            "iter 44200 (p=707200), loss: 2.076597\n",
            "iter 44400 (p=710400), loss: 2.032669\n",
            "iter 44600 (p=713600), loss: 2.105131\n",
            "iter 44800 (p=716800), loss: 2.102445\n",
            "----\n",
            " em\n",
            "Fo nould!\n",
            "\n",
            "CHOFBETCSWich in fod beed, riis a! nond,\n",
            "Yith corn wesw. What coplst anded saian?\n",
            "Weak's wind nit ald'tall terengiss handoldouge.\n",
            "Wey,\n",
            "Of wnows ckoun wigying you lugm:\n",
            "3ommeots ushire\n",
            "To \n",
            "----\n",
            "iter 45000 (p=720000), loss: 2.134964\n",
            "iter 45200 (p=723200), loss: 2.149574\n",
            "iter 45400 (p=726400), loss: 2.125373\n",
            "iter 45600 (p=729600), loss: 2.124894\n",
            "iter 45800 (p=732800), loss: 2.106573\n",
            "----\n",
            " e ar's?\n",
            "\n",
            "Fow sugy wath this of leayess?\n",
            "\n",
            "KING'T::\n",
            "Swer Lices thour dout wit buss twall bourst of me gomban nook a ale; Vy plisy leilllles, whem meare trour,\n",
            "The cow tich theef Glodrsamy seam of to tal \n",
            "----\n",
            "iter 46000 (p=736000), loss: 2.095561\n",
            "iter 46200 (p=739200), loss: 2.077454\n",
            "iter 46400 (p=742400), loss: 2.066342\n",
            "iter 46600 (p=745600), loss: 2.105207\n",
            "iter 46800 (p=748800), loss: 2.108401\n",
            "----\n",
            " twiss.\n",
            "Froce gat been, hangs hene,\n",
            "And a mimir not nonacest unst tont\n",
            "PCagpheod ther he,\n",
            "Ay be my bait the kerewe:\n",
            "Ho cane.\n",
            "An erithce thou hom besol stien', rowhey,\n",
            "Yollill of tuf, mancer.\n",
            "\n",
            "GLETESATE \n",
            "----\n",
            "iter 47000 (p=752000), loss: 2.098931\n",
            "iter 47200 (p=755200), loss: 2.081839\n",
            "iter 47400 (p=758400), loss: 2.139938\n",
            "iter 47600 (p=761600), loss: 2.110337\n",
            "iter 47800 (p=764800), loss: 2.124841\n",
            "----\n",
            " rth unt so nomenou one, sellie to mightifs! that he me suy,\n",
            "pryammy winLurd we\n",
            "Tht mour foce,\n",
            "Thabe bat emears: dove.\n",
            "\n",
            "HESGONgINSSSULES LONE:\n",
            "I day doustly paimby sewand ane mys quting alw:\n",
            "Foref Gea' \n",
            "----\n",
            "iter 48000 (p=768000), loss: 2.112648\n",
            "iter 48200 (p=771200), loss: 2.123077\n",
            "iter 48400 (p=774400), loss: 2.132791\n",
            "iter 48600 (p=777600), loss: 2.129848\n",
            "iter 48800 (p=780800), loss: 2.170368\n",
            "----\n",
            " ? is and, mighale I and sis yourn on hawienint, my se?\n",
            "Yot thour! reitneres, thar an than whithar a o I shead\n",
            "us sheens\n",
            "As a ald; pat so myn'd, sbeep souvis.\n",
            "\n",
            "ALEONEO: yak! I all, salls not I ny, my m \n",
            "----\n",
            "iter 49000 (p=784000), loss: 2.188348\n",
            "iter 49200 (p=787200), loss: 2.198624\n",
            "iter 49400 (p=790400), loss: 2.160246\n",
            "iter 49600 (p=793600), loss: 2.183011\n",
            "iter 49800 (p=796800), loss: 2.193263\n",
            "----\n",
            " mou, moud ul heangce,\n",
            "Thesie dor I ane ink deakins atoad the kive,\n",
            "Proms; prave op to eongle is I malle, woot smares\n",
            "To muth\n",
            "O!\n",
            "Ol you tweans you deeny, dorese:\n",
            "I gay, be.\n",
            "\n",
            "PLCONRETES:\n",
            "It pach?\n",
            "\n",
            "PMIMU \n",
            "----\n",
            "iter 50000 (p=800000), loss: 2.142296\n",
            "iter 50200 (p=803200), loss: 2.097651\n",
            "iter 50400 (p=806400), loss: 2.115533\n",
            "iter 50600 (p=809600), loss: 2.127399\n",
            "iter 50800 (p=812800), loss: 2.115218\n",
            "----\n",
            " op ares whow putelrsen ther but itst angol; He mone:\n",
            "That thoulhoprer.\n",
            "\n",
            "CAMAMLARD IZES:\n",
            "As trow thorgs.\n",
            "\n",
            "CULLIZUER:\n",
            "That witruss: cut not safterd ben.\n",
            "\n",
            "He:\n",
            "Thee ame: w.\n",
            "Thou, wlatlr our ot un\n",
            "is, put  \n",
            "----\n",
            "iter 51000 (p=816000), loss: 2.085111\n",
            "iter 51200 (p=819200), loss: 2.092413\n",
            "iter 51400 (p=822400), loss: 2.105866\n",
            "iter 51600 (p=825600), loss: 2.106651\n",
            "iter 51800 (p=828800), loss: 2.109409\n",
            "----\n",
            " e the and and theads a we dord\n",
            "Yeacher teriot me not have my oo a fered: Dith, sepheinss ha king: cay?\n",
            "O of hime a peverch 'erer as his free cop, I me to d;ro thou?\n",
            "\n",
            "LAUMINS:\n",
            "Cise\n",
            "dich blreane\n",
            "I Ovour \n",
            "----\n",
            "iter 52000 (p=832000), loss: 2.068925\n",
            "iter 52200 (p=835200), loss: 2.065494\n",
            "iter 52400 (p=838400), loss: 2.054168\n",
            "iter 52600 (p=841600), loss: 2.066712\n",
            "iter 52800 (p=844800), loss: 2.073758\n",
            "----\n",
            " de thoud stomawe.\n",
            "I vechen undnthine mistirre, the hes.\n",
            "Lerencul. U'll, plinceve or profes Opouenendy; au,\n",
            "Wheres: his not bredesere sand sens, my plrccaly I it\n",
            "Do a and,\n",
            "ordneghe, lule,\n",
            "Yow wire\n",
            "To a \n",
            "----\n",
            "iter 53000 (p=848000), loss: 2.091581\n",
            "iter 53200 (p=851200), loss: 2.068577\n",
            "iter 53400 (p=854400), loss: 2.054414\n",
            "iter 53600 (p=857600), loss: 2.032398\n",
            "iter 53800 (p=860800), loss: 2.035156\n",
            "----\n",
            " e me'm and? Ewos, oullese far of ofr wist reding mroll, frichare. I beonge com not\n",
            "I and of and beart thy forodespire.\n",
            "\n",
            "FLOZOO:\n",
            "Om tho porcie, now\n",
            "To selays?\n",
            "\n",
            "HAULOS:\n",
            "To meeson be loot bloavimss ' kne \n",
            "----\n",
            "iter 54000 (p=864000), loss: 2.063767\n",
            "iter 54200 (p=867200), loss: 2.077951\n",
            "iter 54400 (p=870400), loss: 2.077981\n",
            "iter 54600 (p=873600), loss: 2.078475\n",
            "iter 54800 (p=876800), loss: 2.097184\n",
            "----\n",
            "  waller.\n",
            "\n",
            "SCOCIAS:\n",
            "We I torpon hed sry not wandthe of thy hath I corsore\n",
            "Aonfoug:\n",
            "Desty a what; co wirkd.\n",
            "\n",
            "Fot deen,\n",
            "I moneed\n",
            "Hid 'lacter.\n",
            "\n",
            "ARILAD:\n",
            "Whet it shou witly.\n",
            "\n",
            "fas I.\n",
            "\n",
            "LAPWIO:\n",
            "Ihy you dormoul \n",
            "----\n",
            "iter 55000 (p=880000), loss: 2.081221\n",
            "iter 55200 (p=883200), loss: 2.075349\n",
            "iter 55400 (p=886400), loss: 2.057252\n",
            "iter 55600 (p=889600), loss: 2.044437\n",
            "iter 55800 (p=892800), loss: 2.033451\n",
            "----\n",
            "  decrooned besting\n",
            "Ae o'ld .\n",
            "O?\n",
            "\n",
            "AUNEN ESTE:\n",
            "Fo an wall has she of je shalper sitheuth as he ay theard aswur,\n",
            "I whal this bowert,\n",
            "The grad pving a\n",
            "I, it the meect me that dousind\n",
            "And and docter ave th \n",
            "----\n",
            "iter 56000 (p=896000), loss: 2.039843\n",
            "iter 56200 (p=899200), loss: 2.025827\n",
            "iter 56400 (p=902400), loss: 2.030376\n",
            "iter 56600 (p=905600), loss: 2.019907\n",
            "iter 56800 (p=908800), loss: 2.064743\n",
            "----\n",
            " this uner po had surdewe. Vome ast gow wack walb gidher.\n",
            "\n",
            "DONCOLIBE:\n",
            "He severes, cot toet's pories lover: the team wede.\n",
            "\n",
            "ARWALUNE:\n",
            "Whelm rae thow\n",
            "erterall be come, be a kid.\n",
            "\n",
            "LINHELUT:\n",
            "Be spilln a Le \n",
            "----\n",
            "iter 57000 (p=912000), loss: 2.076193\n",
            "iter 57200 (p=915200), loss: 2.019516\n",
            "iter 57400 (p=918400), loss: 2.023409\n",
            "iter 57600 (p=921600), loss: 2.004374\n",
            "iter 57800 (p=924800), loss: 2.047003\n",
            "----\n",
            "  we and ce.\n",
            "\n",
            "Ovet pors to mar:\n",
            "got e;\n",
            "detond.\n",
            "Yound mastam go:\n",
            "Ef thile anouvin to-mone frave mes monion wo stitny t she taring meake:\n",
            "\n",
            "VOSCIO:\n",
            "romthis Bume.\n",
            "\n",
            "ACKIVTJD:\n",
            "Proalvece, mang in I me cath\n",
            "le \n",
            "----\n",
            "iter 58000 (p=928000), loss: 2.077875\n",
            "iter 58200 (p=931200), loss: 2.047528\n",
            "iter 58400 (p=934400), loss: 2.054967\n",
            "iter 58600 (p=937600), loss: 2.048006\n",
            "iter 58800 (p=940800), loss: 2.035993\n",
            "----\n",
            " sty t; A' stich preke.\n",
            "\n",
            "SARDIO:\n",
            "Herust srent ipun thildid praar nrive, speotallil luy of bestine onter haim, maip,\n",
            "You the me tous feangicte\n",
            "py chove tir cepeins will war youse highie\n",
            "Ancire; as that  \n",
            "----\n",
            "iter 59000 (p=944000), loss: 2.076682\n",
            "iter 59200 (p=947200), loss: 2.045892\n",
            "iter 59400 (p=950400), loss: 2.043898\n",
            "iter 59600 (p=953600), loss: 2.002381\n",
            "iter 59800 (p=956800), loss: 1.966817\n",
            "----\n",
            " sto him stmee praicune? fith whes mone od, do, ftraysem\n",
            "Thake if wainger!\n",
            "Sonh\n",
            "I\n",
            "Por of rort Irere lestard Hone!\n",
            "ABEd;\n",
            "And siren nos,\n",
            "Mrables?\n",
            "\n",
            "BUKE HANCENENTIO:\n",
            "WAseme rand\n",
            "thigh fronngred, cat prepe \n",
            "----\n",
            "iter 60000 (p=960000), loss: 1.946774\n",
            "iter 60200 (p=963200), loss: 1.936756\n",
            "iter 60400 (p=966400), loss: 1.932363\n",
            "iter 60600 (p=969600), loss: 1.995752\n",
            "iter 60800 (p=972800), loss: 2.021733\n",
            "----\n",
            " wiys!\n",
            "\n",
            "ADFERIPLILA:\n",
            "We dett hem enich yaut a preave\n",
            "Ank\n",
            "Row math not ding poddie, thane ligh ofd: do thim thou thy beer have the beed, in ford mun whend the a pord\n",
            "wholl wooll fortens'l an' wath gyous \n",
            "----\n",
            "iter 61000 (p=976000), loss: 2.022816\n",
            "iter 61200 (p=979200), loss: 2.015712\n",
            "iter 61400 (p=982400), loss: 2.038332\n",
            "iter 61600 (p=985600), loss: 2.096682\n",
            "iter 61800 (p=988800), loss: 2.091582\n",
            "----\n",
            " t prieg mosteder sid this kim lyortant,\n",
            "And bust will comrey, stillesonaly!\n",
            "\n",
            "KUHEON:\n",
            "Ywuld, fay? I then, wnUke theon-\n",
            "Of sereic havine nrell-sorseve\n",
            "Blest ciet\n",
            "Seakid, stall owalked stis knot\n",
            "As a ked \n",
            "----\n",
            "iter 62000 (p=992000), loss: 2.094370\n",
            "iter 62200 (p=995200), loss: 2.090860\n",
            "iter 62400 (p=998400), loss: 2.098082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "czVXA6mmFIdX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}